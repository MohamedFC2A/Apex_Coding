# NEXUS AI CODING - Environment Configuration
# Copy this file to .env and update with your actual values

# ==============================================
# App / Server Configuration
# ==============================================
NODE_ENV=development
PORT=3001
# Comma-separated allowed origins for CORS (recommended in production)
# FRONTEND_URL="https://your-app.vercel.app"
# FRONTEND_ORIGIN="https://your-app.vercel.app"

# ==============================================
# LLM Configuration (DeepSeek V3.2)
# ==============================================
LLM_PROVIDER="deepseek"
DEEPSEEK_BASE_URL="https://api.deepseek.com"
DEEPSEEK_API_KEY="sk-YOUR_KEY_HERE"

# Model Defaults (DeepSeek V3.2)
# - Use `deepseek-chat` for normal coding (fast mode)
# - When "Thinking Mode" is enabled, backend switches to `DEEPSEEK_THINKING_MODEL` (default: deepseek-reasoner)
DEEPSEEK_MODEL="deepseek-chat"
# Optional explicit overrides (preferred if you want full control):
# DEEPSEEK_FAST_MODEL="deepseek-chat"
# DEEPSEEK_THINKING_MODEL="deepseek-reasoner"

# ==============================================
# Frontend (Vercel -> Backend on Vercel)
# ==============================================
# Vercel frontend calls the backend over HTTPS.
VITE_BACKEND_URL="https://apex-coding-backend.vercel.app"

# ==============================================
# (Optional / Legacy) Frontend-only DeepSeek
# ==============================================
# If you *intentionally* run the LLM directly in the browser, set these VITE_DEEPSEEK_* vars.
# WARNING: This exposes the API key to end users.
# VITE_DEEPSEEK_BASE_URL="https://api.deepseek.com"
# VITE_DEEPSEEK_MODEL="deepseek-chat"
# VITE_DEEPSEEK_THINKING_MODEL="deepseek-reasoner"
# VITE_DEEPSEEK_API_KEY="sk-YOUR_KEY_HERE"

# ==============================================
# Security & Execution Limits
# ==============================================
MAX_EXECUTION_TIME=30000
CPU_LIMIT=70
RAM_LIMIT=60

# ==============================================
# API Rate Limiting (optional)
# ==============================================
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX=100

# ==============================================
# Instructions
# ==============================================
# 1. Rename this file to .env
# 2. Set DEEPSEEK_API_KEY to your actual API key
# 3. Never commit .env to version control (already in .gitignore)
#
# DeepSeek V3.2 Notes:
# - Fast mode (deepseek-chat): Standard OpenAI-compatible API calls
# - Thinking mode (deepseek-reasoner): Chain-of-thought reasoning
#   * Do NOT send temperature, top_p, or presence_penalty params
#   * Filter out reasoning_content from previous assistant messages
#   * Capture reasoning_content from response for chain-of-thought display

# ==============================================
# WebContainer (optional)
# ==============================================
# Used by @webcontainer/api to authenticate and avoid rate limiting.
VITE_WC_CLIENT_ID="wc_api_YOUR_KEY_HERE"
