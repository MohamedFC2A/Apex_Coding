import OpenAI from 'openai';
import { ProjectFile } from '@/types';
import { getLanguageFromExtension } from '@/utils/stackDetector';

interface AIResponse {
  plan: string;
  decisionTrace?: string;
  files: ProjectFile[];
  fileStructure: { path: string; type: 'file' }[];
  stack: string;
  description: string;
}

const FILE_NODE_RULES = [
  'FILE NODE RULES (MANDATORY):',
  '- In each project_files entry, output the "name" field before "content".',
  '- Never emit file content before the file name is declared.',
  '- If a file is named without a folder (e.g. "style.css"), decide frontend/ vs backend/ and include the full path.',
  '- Declare folder-prefixed paths as soon as you decide them so the UI can materialize folders in real time.',
  '- OVERWRITE MODE: Always output full file contents from scratch; never assume prior content will be appended.'
].join('\n');

const COPYRIGHT_FOOTER_RULES = [
  'COPYRIGHT FOOTER RULE (MANDATORY):',
  '- Every generated frontend must include a small footer showing a copyright notice and "Generated by NEXUS AI CODING".',
  '- Example footer text: "© Matany — Generated by NEXUS AI CODING".'
].join('\n');

const WEB_CONTAINER_RULES = [
  'WEB CONTAINER FULL-STACK REQUIREMENTS:',
  '- You are a Full-Stack Generator using WebContainers.',
  '- Always generate a folder structure with /backend and /frontend.',
  '- Backend must use express with sqlite3 or better-sqlite3. Never use MongoDB Atlas.',
  '- Backend must listen on port 3000 or 3111.',
  '- You MUST generate a root-level package.json at /package.json (project root).',
  '- Root /package.json MUST include scripts:',
  '  - "start": "concurrently \\"npm run server\\" \\"npm run client\\""',
  '  - "server": "cd backend && node server.js"',
  '  - "client": "cd frontend && vite"',
  '- Root /package.json MUST include dependencies: concurrently, express, sqlite3, cors, nodemon.',
  '- Root /package.json MUST match this template (fill only safe extras):',
  '{',
  '  "name": "project-root",',
  '  "scripts": {',
  '    "start": "concurrently \\\\\\"npm run server\\\\\\" \\\\\\"npm run client\\\\\\"",',
  '    "server": "cd backend && node server.js",',
  '    "client": "cd frontend && vite"',
  '  },',
  '  "dependencies": {',
  '    "concurrently": "^8.0.0",',
  '    "express": "^4.18.0",',
  '    "sqlite3": "^5.1.0",',
  '    "cors": "^2.8.5",',
  '    "nodemon": "^3.0.0"',
  '  }',
  '}',
  '- Ensure frontend/index.html includes Tailwind CDN in <head>: <script src="https://cdn.tailwindcss.com"></script>.',
  '- Never output plain unstyled HTML. Use modern premium classes (e.g. bg-slate-900 text-white, glass-effect).',
  '',
  'BACKEND SERVE RULE (MANDATORY):',
  '- backend/server.js MUST serve the frontend files to avoid "Cannot GET /".',
  '- Add exactly this (CommonJS) snippet to backend/server.js:',
  '  const path = require("path");',
  '  app.use(express.static(path.join(__dirname, "../frontend")));',
  '  app.get("*", (req, res) => {',
  '    res.sendFile(path.join(__dirname, "../frontend/index.html"));',
  '  });'
].join('\n');

const ARCHITECT_MODE_RULES = [
  'ARCHITECT MODE OUTPUT ORDER:',
  '- Start by declaring the folder structure through file paths (package.json, frontend/, backend/, README.md) before any file content streams.',
  '- In Architect Mode, declare /package.json first (root), then backend/, then frontend/.',
  '- Keep UI code under frontend/ and server code under backend/.'
].join('\n');

const PLAN_SYSTEM_PROMPT = `You are an Architect planning engine.

Return ONLY valid JSON with this exact shape:
{
  "steps": [
    { "id": "1", "title": "Short actionable step" }
  ]
}

RULES:
- Output ONLY JSON (no markdown, no code fences, no extra text)
- 6 to 12 steps max
- Each title must be concise (max ~70 chars), imperative, and implementation-oriented
- Do not include file contents; only describe steps
- Plan for a full-stack split with frontend/ + backend/ and root package.json
`;

const extractUrl = (raw: string) => {
  const match = raw.match(/https?:\/\/[^\s)]+/i);
  return match?.[0] || raw;
};

const normalizeBaseURL = (raw: string | undefined, fallback: string) => {
  const input = extractUrl(String(raw ?? fallback)).trim().replace(/^"+|"+$/g, '');
  const withoutSlash = input.replace(/\/+$/, '');
  return withoutSlash.endsWith('/v1') ? withoutSlash : `${withoutSlash}/v1`;
};

const getApiKey = () => {
  const key = (import.meta as any)?.env?.VITE_DEEPSEEK_API_KEY as string | undefined;
  if (!key || key.trim().length === 0) {
    throw new Error('API Key not configured in Settings');
  }
  return key.trim();
};

const getModel = (thinkingMode: boolean) => {
  const env = (import.meta as any)?.env || {};
  if (thinkingMode) return (env.VITE_DEEPSEEK_THINKING_MODEL as string) || 'deepseek-reasoner';
  return (env.VITE_DEEPSEEK_MODEL as string) || (env.VITE_DEEPSEEK_FAST_MODEL as string) || 'deepseek-chat';
};

let cachedClient: OpenAI | null = null;
const getClient = () => {
  if (cachedClient) return cachedClient;

  const env = (import.meta as any)?.env || {};
  const baseURL = normalizeBaseURL(env.VITE_DEEPSEEK_BASE_URL, 'https://api.deepseek.com');
  const apiKey = getApiKey();

  cachedClient = new OpenAI({
    baseURL,
    apiKey,
    dangerouslyAllowBrowser: true
  });

  return cachedClient;
};

const tryParseJSON = (raw: string) => {
  const trimmed = raw.trim();
  if (trimmed.length === 0) throw new Error('Empty AI response');

  const noFences = trimmed
    .replace(/^```(?:json)?/i, '')
    .replace(/```$/i, '')
    .trim();

  try {
    return JSON.parse(noFences);
  } catch {
    const start = noFences.indexOf('{');
    const end = noFences.lastIndexOf('}');
    if (start >= 0 && end > start) {
      const slice = noFences.slice(start, end + 1);
      return JSON.parse(slice);
    }
    throw new Error('Failed to parse JSON response');
  }
};

const buildRules = (architectMode: boolean) =>
  architectMode
    ? `${FILE_NODE_RULES}\n${COPYRIGHT_FOOTER_RULES}\n${WEB_CONTAINER_RULES}\n${ARCHITECT_MODE_RULES}`
    : `${FILE_NODE_RULES}\n${COPYRIGHT_FOOTER_RULES}\n${WEB_CONTAINER_RULES}`;

export const aiService = {
  async generatePlan(prompt: string, thinkingMode: boolean = false): Promise<{ steps: Array<{ id: string; title: string }> }> {
    const client = getClient();
    const model = getModel(thinkingMode);

    const completion = await client.chat.completions.create({
      model,
      temperature: 0.0,
      response_format: { type: 'json_object' },
      messages: [
        { role: 'system', content: PLAN_SYSTEM_PROMPT },
        { role: 'user', content: prompt }
      ]
    });

    const content = completion.choices?.[0]?.message?.content ?? '';
    const parsed = tryParseJSON(content);

    const stepsRaw = Array.isArray(parsed) ? parsed : parsed?.steps;
    const steps = Array.isArray(stepsRaw)
      ? stepsRaw
          .map((s: any, index: number) => ({
            id: String(s?.id ?? index + 1),
            title: String(s?.title ?? s?.text ?? s?.step ?? '').trim()
          }))
          .filter((s) => s.title.length > 0)
      : [];

    return { steps };
  },

  async generateCode(prompt: string): Promise<AIResponse> {
    const client = getClient();
    const model = getModel(false);
    const rules = buildRules(false);

    const completion = await client.chat.completions.create({
      model,
      temperature: 0.0,
      messages: [
        { role: 'system', content: rules },
        { role: 'user', content: prompt }
      ]
    });

    const content = completion.choices?.[0]?.message?.content ?? '';
    const data: any = tryParseJSON(content);

    if (data?.files) {
      data.files = data.files.map((file: ProjectFile) => ({
        ...file,
        language: file.language || getLanguageFromExtension(file.path || file.name || '')
      }));
    }

    return data as AIResponse;
  },

  async generateCodeStream(
    prompt: string,
    onToken: (token: string) => void,
    onStatus: (phase: string, message: string) => void,
    onMeta: (meta: any) => void,
    onJSON: (payload: any) => void,
    onError: (error: string) => void,
    onReasoning: (chunk: string) => void,
    onComplete: () => void,
    thinkingModeOrOptions:
      | boolean
      | {
          thinkingMode?: boolean;
          architectMode?: boolean;
          includeReasoning?: boolean;
          history?: any[];
        } = false
  ): Promise<void> {
    try {
      const options = typeof thinkingModeOrOptions === 'boolean' ? { thinkingMode: thinkingModeOrOptions } : thinkingModeOrOptions;
      const thinkingMode = Boolean(options.thinkingMode);
      const architectMode = Boolean(options.architectMode);
      const includeReasoning = Boolean(options.includeReasoning);

      const client = getClient();
      const model = getModel(thinkingMode);
      const env = (import.meta as any)?.env || {};
      const baseURL = normalizeBaseURL(env.VITE_DEEPSEEK_BASE_URL, 'https://api.deepseek.com');

      onMeta({ provider: 'deepseek', model, baseURL, thinkingMode, architectMode });
      onStatus(thinkingMode ? 'thinking' : 'streaming', thinkingMode ? 'Thinking…' : 'Generating…');

      const rules = buildRules(architectMode);
      const messages = [
        { role: 'system' as const, content: rules },
        { role: 'user' as const, content: prompt }
      ];

      const stream = await client.chat.completions.create({
        model,
        stream: true,
        temperature: 0.0,
        messages
      });

      let fullText = '';

      for await (const part of stream as any) {
        const delta = part?.choices?.[0]?.delta || {};

        const reasoning = delta?.reasoning_content;
        if (includeReasoning && typeof reasoning === 'string' && reasoning.length > 0) {
          onReasoning(reasoning);
        }

        const chunk = delta?.content;
        if (typeof chunk === 'string' && chunk.length > 0) {
          fullText += chunk;
          onToken(chunk);
        }
      }

      onStatus('done', 'Complete');

      const payload = tryParseJSON(fullText);
      onJSON(payload);
      onComplete();
    } catch (err: any) {
      onError(err?.message || 'Streaming failed');
      onComplete();
    }
  }
};

